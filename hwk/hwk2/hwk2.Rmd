---
title: 'STAT5514: Homework 2'
author: "Alexander Durbin"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
options(stringsAsFactors = FALSE, scipen = 1E8)
```

```{r, include = FALSE}

library(tidyverse)
library(tibble)
library(MASS)
library(quantreg)
library(galts)
library(knitr)

```

```{r, eval = TRUE, echo = FALSE}

b0_sim <- tibble(ls1 = rep(0, 100),
                 ls2 = rep(0, 100),
                 ls3 = rep(0, 100),
                 hb1 = rep(0, 100),
                 hb2 = rep(0, 100),
                 hb3 = rep(0, 100),
                 ld1 = rep(0, 100),
                 ld2 = rep(0, 100),
                 ld3 = rep(0, 100),
                 lt1 = rep(0, 100),
                 lt2 = rep(0, 100),
                 lt3 = rep(0, 100))

b1_sim <- tibble(ls1 = rep(0, 100),
                 ls2 = rep(0, 100),
                 ls3 = rep(0, 100),
                 hb1 = rep(0, 100),
                 hb2 = rep(0, 100),
                 hb3 = rep(0, 100),
                 ld1 = rep(0, 100),
                 ld2 = rep(0, 100),
                 ld3 = rep(0, 100),
                 lt1 = rep(0, 100),
                 lt2 = rep(0, 100),
                 lt3 = rep(0, 100))

b0 <- 1
b1 <- 2

for(i in 1:100){
  raw <- tibble(x = rnorm(100, 0, 1), 
                e1 = rnorm(100, 0, 2),
                e2 = rlnorm(100, 0, 2),
                e3 = stats::rcauchy(100, 0 ,2),
                y1 = b0 + b1 * x + e1,
                y2 = b0 + b1 * x + e2,
                y3 = b0 + b1 * x + e3)
  
  
  
  l1 <- lm(y1 ~ x, data = raw)
  l2 <- lm(y2 ~ x, data = raw)
  l3 <- lm(y3 ~ x, data = raw)
  
  b0_sim$ls1[i] = coefficients(l1)[1]
  b1_sim$ls1[i] = coefficients(l1)[2]
  
  b0_sim$ls2[i] = coefficients(l2)[1]
  b1_sim$ls2[i] = coefficients(l2)[2]
  
  b0_sim$ls3[i] = coefficients(l3)[1]
  b1_sim$ls3[i] = coefficients(l3)[2]
  
  l1 <- rlm(y1 ~ x, data = raw)
  l2 <- rlm(y2 ~ x, data = raw)
  l3 <- rlm(y3 ~ x, data = raw)
  
  b0_sim$hb1[i] = coefficients(l1)[1]
  b1_sim$hb1[i] = coefficients(l1)[2]
  
  b0_sim$hb2[i] = coefficients(l2)[1]
  b1_sim$hb2[i] = coefficients(l2)[2]
  
  b0_sim$hb3[i] = coefficients(l3)[1]
  b1_sim$hb3[i] = coefficients(l3)[2]
  
  l1 <- rq(y1 ~ x, data = raw)
  l2 <- rq(y2 ~ x, data = raw)
  l3 <- rq(y3 ~ x, data = raw)
  
  b0_sim$ld1[i] = coefficients(l1)[1]
  b1_sim$ld1[i] = coefficients(l1)[2]
  
  b0_sim$ld2[i] = coefficients(l2)[1]
  b1_sim$ld2[i] = coefficients(l2)[2]
  
  b0_sim$ld3[i] = coefficients(l3)[1]
  b1_sim$ld3[i] = coefficients(l3)[2]

  l1 <- ga.lts(raw$y1 ~ raw$x, lower = -100, upper = 100)
  l2 <- ga.lts(raw$y2 ~ raw$x, lower = -100, upper = 100)
  l3 <- ga.lts(raw$y3 ~ raw$x, lower = -100, upper = 100)
  
  b0_sim$lt1[i] = coefficients(l1)[1]
  b1_sim$lt1[i] = coefficients(l1)[2]
  
  b0_sim$lt2[i] = coefficients(l2)[1]
  b1_sim$lt2[i] = coefficients(l2)[2]
  
  b0_sim$lt3[i] = coefficients(l3)[1]
  b1_sim$lt3[i] = coefficients(l3)[2]
 
}


```


```{r, eval = TRUE, echo = FALSE}

b0_s_m <- colMeans(b0_sim)
b1_s_m <- colMeans(b1_sim)

b0_bias <- (b0 - b0_s_m)
b1_bias <- (b1 - b1_s_m)

b0_bias_sq <- (b0 - b0_s_m)^2
b1_bias_sq <- (b1 - b1_s_m)^2

b0_var <- sapply(b0_sim, var)
b1_var <- sapply(b1_sim, var)

b0_mse <- b0_bias_sq + b0_var

b1_mse <- b1_bias_sq + b1_var
```

```{r, eval = TRUE, echo=FALSE}

knitr::kable(as.data.frame(matrix(b0_var,nrow = 3), row.names = c('Normal', 'Lognormal', 'Cauchy')), col.names = c('LSE', 'RLM', 'LAD', 'LTS'), caption = 'Variance of intercept estimates.')

knitr::kable(as.data.frame(matrix(b1_var,nrow = 3), row.names = c('Normal', 'Lognormal', 'Cauchy')), col.names = c('LSE', 'RLM', 'LAD', 'LTS'), caption = 'Variance of slope estimates.')

knitr::kable(as.data.frame(matrix(b0_bias,nrow = 3), row.names = c('Normal', 'Lognormal', 'Cauchy')), col.names = c('LSE', 'RLM', 'LAD', 'LTS'), caption = 'Bias of intercept estimates.')

knitr::kable(as.data.frame(matrix(b1_bias,nrow = 3), row.names = c('Normal', 'Lognormal', 'Cauchy')), col.names = c('LSE', 'RLM', 'LAD', 'LTS'), caption = 'Bias of slope estimates.')

knitr::kable(as.data.frame(matrix(b0_mse,nrow = 3), row.names = c('Normal', 'Lognormal', 'Cauchy')), col.names = c('LSE', 'RLM', 'LAD', 'LTS'), caption = 'MSE of intercept estimates.')

knitr::kable(as.data.frame(matrix(b1_mse,nrow = 3), row.names = c('Normal', 'Lognormal', 'Cauchy')), col.names = c('LSE', 'RLM', 'LAD', 'LTS'), caption = 'MSE of slope estimates.')

```

# Normal Error - LSE, RLM

As expected for normally distributed error, lease squares performs best with minimum mean squarred error. Why is best understood using linear models theory. Under normally distributed error and constant variance, we know that least squares estimators are unbiased and have minimum variance amongst all unbiased linear estimators by Gauss-Markov Theorem.

Robust regression performs next best with results very comparable to least squares. Robust regression is most effective when there are several large residuals so that it may appropriately downweight these observations. But for normally distributed error with low variance, there are few outliers. It must be the case that the majority of observations are given full weight so that during the iterative reweighted least squares procedure, we are essentially performing least squares estimation.

# Lognormal Error - LTS, LAD

# Cauchy Error - LAD, RLM/LTS (depends on Cauchy draw)