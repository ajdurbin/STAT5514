---
title: "Homework 3"
subtitle: 'STAT5514: Regression Analysis'
author: "Alexander Durbin"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Summary

This paper is concerned with the bias that results from transforming non-linear data. The author considers cases where $X, Y$, or both are transformed such that residual assumptions are satisfied for linear regression. The author notes that after fitting a linear model, we can apply the inverse transformation to get confidence intervals and predicition intervals. However, if we are concerned with the mean response of $Y$ given $X$, data transformation can result in large bias of models. This paper suggests solutions for bias induced from common data transformations. The author considers four cases of transformation. Each case includes a bias-adjustment factor that eliminates a large portion of bias for each model. Furthermoer, the resulting bias-adjusted estimates are still slightly biased because the data transformations consist of non-linear functions of parameters. The paper concludes with a short discussion on more advanced techniques for reducing bias and notes that these techniques may not be very useful in practice.

# Impressions

Overall, I thought this paper discussed a very relevant problem in my statistical education thus far. While theoretically we can focus on estiamtes that are BLUE or UMVUE, in practice data is not that well-behaved. Instead, the author focuses on common transformations and how to reduce their bias. Though discussing very applicable results, I think including a simulation study beyond the logarithm section would be useful. I found this paper accessible, though a little too concise mathematically. 

# Criticisms

- Typos in derivations. When discussing transformations using positive fractional powers, Equation 4.9 should be $E(Y) = (\beta_0 + \beta_1 X)^2 + \sigma^2$, not $E(Y) = (\beta_0 + \beta_1 X) + \sigma^2$.

- While the author provided excellent motivation for the topic and mentions that the more advanced techniques are not useful practically, it would have been beneficial to see a simulation study to analytically illustrate this.

- The paragraph following Equation 4.12 should derive the geometric progression and truncation and not just describe it. Furthermore, how this results in $\sigma^2$ in the numerator of Equation 4.13. 

- In logarithm and positive fractional power transformations, the bias-adjustment factor is simply the variance estimate, $\hat{\sigma}^2$. But in inverse transformations, the bracketed term in Equation 4.13 is the bias-adjustment factor, it is not clear whether this is simply because this term contains $\sigma^2$ or not.

- Similar to above, the author does not state what the bias-adjustment factor is in Equation 4.14. Both terms contain $\sigma^2$ here. Again, if the bias-adjustment factor is simply $\sigma^2$, this point is moot.

- The author does not discuss or mention how these results are affected in multiple regression.

- Equation 2.2 is incorrect, $Y = 1480.3 \times X^{0.5}$ since $e^{7.3} = 1480.3 \ne 1500$.

- Typo in paragraph following Equation 4.6, $\hat{ln(Y)}$ should be $ln(\hat{Y})$.

# Bias Results

## Logarithm
Assuming the true underlying model is $ln(Y) = \beta_0 + \beta_1X + \epsilon$, we see that $E(Y) = exp(\beta_0 + \beta_1X + \frac{1}{2}\sigma^2)$ using lognormal properties. Our fitted model is $ln(\hat{Y}) = \hat{\beta}_0 + \hat{\beta}_1X$. $ln(\hat{Y})$ and has mean $\beta_0 + \beta_1X$ and variance $\sigma^2(\frac{1}{n} + \frac{(x-\bar{x})^2}{S_{xx}})$, so that $\hat{Y}$ is also distributed lognormal with mean $E(\hat{Y}) = exp(\beta_0 + \beta_1X + \frac{1}{2}\sigma^2[\frac{1}{n}+\frac{(x-\bar{x})^2}{S_{xx}}])$. Thus we see that the bias is 

\begin{align*}
E(Y - \hat{Y}) &= exp \Bigg\{ \beta_0 + \beta_1X + \frac{1}{2}\sigma^2 \Bigg\} - exp \Bigg\{ \beta_0 + \beta_1X + \frac{1}{2}\sigma^2[\frac{1}{n} + \frac{(x-\bar{x})^2}{S_{xx}}] \Bigg\} \\
&\ne 0.
\end{align*}

## Postive Fractional Powers

Assuming the true underlying model is $Y^\frac{1}{n} = \beta_0 + \beta_1X + \epsilon$, we use normal moment generating function properties to see that for $n=2$, $E(Y) = (\beta_0 + \beta_1X)^2 +\sigma^2$. Our fitted model is $\hat{Y}^\frac{1}{n} = \hat{\beta}_0 + \hat{\beta_1}X$, so that similar to above, $E(\hat{Y}) =  (\beta_0 + \beta_1X)^2 +\sigma^2[\frac{1}{n}+\frac{(x-\bar{x})^2}{S_{xx}}]$. The bias is then 

\begin{align*}

E(Y-\hat{Y}) &=  (\beta_0 + \beta_1X)^2 + \sigma^2 - (\beta_0 + \beta_1X)^2 - \sigma^2 \Bigg[ \frac{1}{n}+\frac{\bar{x}^2}{S_{xx}} + \frac{x^2}{S_{xx}} \Bigg] \\

&= \sigma^2 \Bigg[1 - \frac{1}{n} - \frac{(x-\bar{x})^2}{S_{xx}}\Bigg] \\
& \ne 0.

\end{align*}

## Inverse

Our true underlying model is $\frac{1}{Y} = \beta_0+\beta_1X+\epsilon$. We then use the identity in the paper to approximate $E(Y) = \frac{1}{\beta_0+\beta_1X} \Big\{ 1 + \frac{\sigma^2}{(\beta_0+\beta_1X)^2} \Big\}$. If we fit $\frac{1}{\hat{Y}} = \hat{\beta}_0+\hat{\beta}_1X$, then using the same identity, we see $E(\hat{Y}) = \frac{1}{\beta_0+\beta_1X} \Big\{ 1 + \frac{\sigma_\beta^2}{(\beta_0+\beta_1X)^2} \Big\}$, where $\sigma_\beta^2 = \sigma^2[\frac{1}{n}+\frac{(x-\bar{x})^2}{S_{xx}}]$. It is easy to see that the bias is nonzero also. Indeed

\[E(Y - \hat{Y}) = \frac{1}{\beta_0+\beta_1X} \Bigg( \frac{\sigma^2 - \sigma_\beta^2}{(\beta_0+\beta_1X)^2} \Bigg) \ne 0. \]

## Inverse Fractional Powers

