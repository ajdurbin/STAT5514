---
title: "Project 1"
subtitle: "Regression Analysis"
author: "Alexander Durbin"
date: "`r Sys.Date()`"
output: html_document
abstract: "This paper concerns itself with nonlinear regression. We look at two different datasets where either the true underlying model is known or unknown. We compare multiple models with the deviance measure, AIC, and BIC. We then make inference with boostrap confidence intervals. On this basis we choose a best fitting model."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
```

```{r, include=FALSE}
options(stringsAsFactors = FALSE)
library(minpack.lm)
library(nls2)
library(mgcv)
library(robustbase)
library(faraway)
library(lawstat)
library(lmtest)
library(splines)
```

# Problem 1

## Introduction

Our data concists of the efficiency for krigging of different soil attributes as a function of the spatial correlation range. Specific soil attributes are 1:1 soil water mixture, extractable soil phosphorus, calcium extractable, cation exchange capacity, and lime and P fertilizer recommendations. We begin with a scatter plot and simple regression diagnostics.

```{r}

raw <- read.csv("EffRange.csv")
raw <- raw[, 2:ncol(raw)]
obs <- raw[, 1:2]
colnames(obs) <- c('y', 'x')

# simple linear regression fit
lmfit <- lm(y ~ x, data = obs)
cook <- cooks.distance(lmfit)

```

```{r, fig.cap="Scatter plot of Range versus Efficiency. There is clearly a nonlinear relationship between the two variables."}

plot(obs$x, obs$y, xlab = "Range", ylab = "Efficiency", main = "Scatter plot of Range Versus Efficiency")

```

```{r, fig.cap="Histogram of residuals from linear fit. The residuals do not appear normally distributed."}

hist(residuals(lmfit), main = "Residuals From Linear Fit", xlab = "Residuals")

```

```{r}

runs.test(residuals(lmfit))

```

Result of Runs Test. We fail to reject the null, so the residuals are iid.

```{r}

shapiro.test(residuals(lmfit))

```

Result of Shapiro-Wilk test for normality. Surprisingly, the residuals are normally distributed.

```{r}

bptest(lmfit)

```

Additionally, the data satisfy constant variance.

```{r, fig.cap="Various Linear Model Diagnostic Plots."}

par(mfrow=c(2,2))
halfnorm(cook, 3, ylab = "Cooks Distance", main = "Residual Cooks Distance")
plot(fitted(lmfit), residuals(lmfit), xlab = "Fitted", ylab = "Residuals", main = "Residuals Versus Fitted")
abline(h=0)
plot(fitted(lmfit), abs(residuals(lmfit)), xlab = "Fitted", ylab = "|Residuals|", main = "|Residuals| Versus Fitted")
abline(h=0)
qqnorm(residuals(lmfit))
qqline(residuals(lmfit))

```

In the simple linear regression setting, it appears that we do have a single outlier. Furthermore, all of our simple linear regression diagnostics hold. So how well does a simple linear regression line fit? 

```{r, fig.cap="Fitted regression line predicting Efficiency from Range. We see that the regression line does not follow the curvature."}

summary(lmfit)
paste0("Deviance: ", deviance(lmfit))
paste0("AIC: ", AIC(lmfit))
paste0("BIC: ", BIC(lmfit))
plot(obs$x, obs$y, xlab = "Range", ylab = "Efficiency", main = "Scatter plot of Range Versus Efficiency")
abline(lmfit)

```

From the linear model summary output, we see that neither coefficients are significant, the F-statistic is not significant, and the Adjusted R-squared is very poor. We also get a deviance measure of the model fit with the AIC and BIC criterion. 

The above results show that a linear model is not an appropriate choice for this data.

### Model 1

\[
y = \frac{\beta_0}{1 + exp(\beta_1 + \beta_2x)} + \epsilon
\]

The first model we consider is an exponential growth model. It has several nice properties that suggest it is an appropriate choice and is similar to a logistic model. Here, $\beta_0$ is the horizontal asymptote, $\beta_1$ is the y-intercept, and $\beta_2$ is the growth rate. We make visual guesses of these three parameters before fitting the nonlinear regression based on this model. We give three measures of model fit and confidence intervals for the coefficients. 

```{r, fig.cap="Nonlinear regression model fit for the population growth model."}

# storage
b_coef <- matrix(data = NA, ncol = 3, nrow = 2000)
ss_res <- rep(0, 2000)

for(i in 1:2000){

  boot <- sample(1:7, 7, replace = TRUE)
  boot_sample <- obs[boot, ]

  try({

    # logistic growth model
    nlfit <- nls2(formula = y ~ a / (1 + exp(b + k * x)),
                  start = list(a = 100, b = 2.5, k = -0.1),
                  data = boot_sample,
                  control = nls.control(maxiter = 1e3,
                                    minFactor = .Machine$double.eps,
                                    tol = 1e-5),
                  algorithm = "default")
    
    b_coef[i, ] <- coefficients(nlfit)
    ss_res[i] <- sum(residuals(nlfit)^2)
    
  }, silent = TRUE)

}

b_coef <- unique(na.omit(b_coef))
ss_res <- unique(na.omit(ss_res))

best <- nls2(formula = y ~ a / (1 + exp(b + k * x)),
             start = list(a = 100, b = 2.5, k = -0.1),
             data = obs,
             control = nls.control(maxiter = 1e3,
                                    minFactor = .Machine$double.eps,
                                    tol = 1e-5),
             algorithm = "default")

# function for predictions
my_func1 <- function(boot_co, dat){
  
  a <- mean(boot_co[, 1])
  b <- mean(boot_co[, 2])
  c <- mean(boot_co[, 3])
  pckg <- a / (1 + exp(b + c*dat))
  return(pckg)
  
}

# plots
pts <- seq(min(obs$x), max(obs$x), by = 0.01)
preds <- my_func1(boot_co = b_coef, dat = pts)
plot(obs$x, obs$y, xlab = "Range", ylab = "Efficiency", main = "Population Growth Model Fit")
lines(pts, preds, lty = 1, col = "red", lwd = 3)

```

```{r}

summary(best)

```

We see that the `a` parameter is statistically significant when we fit the nonlinear regression on the entire dataset with this assumed model. Furthermore, the fitted nonlinear regression line appears to fit well to the data.

Our bootstrap parameter estimates are as follows.

```{r}

paste0("Bootstrapped Mean Coefficient, beta_0: ", mean(b_coef[, 1]))
paste0("Bootstrapped Mean Coefficient, beta_1: ", mean(b_coef[, 2]))
paste0("Bootstrapped Mean Coefficient, beta_2: ", mean(b_coef[, 3]))

```

We see that the bootstrap parameter estimates are comparable to those of the model fit with the entire dataset.

The bootstrap confidence intervals are the following.

```{r}

paste0("beta_0 bootstrap ci: ", quantile(b_coef[, 1], probs = c(0.025, 0.975)))
paste0("beta_1 bootstrap ci: ", quantile(b_coef[, 2], probs = c(0.025, 0.975)))
paste0("beta_2 bootstrap ci: ", quantile(b_coef[, 3], probs = c(0.025, 0.975)))

```

We see that none of the bootstrap confidence intervals contain zero, so that the bootstrap coefficients are significantly different from zero. Note that the `a` parameter is statistically significant in the model fit with the entire dataset.

The following code gives measures of model fit.

```{r}

paste0("Deviance: ", deviance(best))
paste0("AIC: ", AIC(best))
paste0("BIC: ", BIC(best))

```

### Model 2

\[
y = \frac{\beta_0(x^2+x\beta_1)}{x^2+x\beta_2+\beta_3} + \epsilon
\]

This model was chosen based on nonlinear example datasets from the *Information Technology Laboratory* website. It appear does also appear to follow a similar logistic regression path. We fit this model and give measurements of fit as well as confidence intervals.

```{r, fig.cap="Nonlinear regression model fit for the modified population growth model."}

# storage
b_coef <- matrix(data = NA, ncol = 4, nrow = 2000)
ss_res <- rep(0, 2000)

for(i in 1:2000){

  boot <- sample(1:7, 7, replace = TRUE)
  boot_sample <- obs[boot, ]

  try({

    # logistic growth model
    nlfit <- nls2(formula = y ~ a*(x^2+b*x)/(x^2+x*d+e),
             start = list(a = 32.6, b = -40, d = -63, e = 1650),
             data = boot_sample,
             control = nls.control(maxiter = 1e3,
                                    minFactor = .Machine$double.eps,
                                    tol = 1e-5),
             algorithm = "default")
    
    b_coef[i, ] <- coefficients(nlfit)
    ss_res[i] <- sum(residuals(nlfit)^2)
    
  }, silent = TRUE)

}

b_coef <- unique(na.omit(b_coef))
ss_res <- unique(na.omit(ss_res))

best <- nls2(formula = y ~ a*(x^2+b*x)/(x^2+x*d+e),
             start = list(a = 32.6, b = -40, d = -63, e = 1650),
             data = obs,
             control = nls.control(maxiter = 1e3,
                                    minFactor = .Machine$double.eps,
                                    tol = 1e-5),
             algorithm = "default")

# function for predictions
my_func1 <- function(boot_co, dat){

  a <- mean(boot_co[, 1])
  b <- mean(boot_co[, 2])
  d <- mean(boot_co[, 3])
  e <- mean(boot_co[, 4])
  pckg <- a*(dat^2 + dat*b) / (dat^2 + dat*d+e)
  return(pckg)

}

# plots
pts <- seq(min(obs$x), max(obs$x), by = 0.01)
preds <- my_func1(boot_co = b_coef, dat = pts)
plot(obs$x, obs$y, xlab = "Range", ylab = "Efficiency", main = "Modified Population Growth Model Fit")
# abline(best)
lines(pts, preds, lty = 1, col = "red", lwd = 3)

```

```{r}

summary(best)

```

We see that only the `b` parameter is statistically significant when this model is fit to the entire dataset. Furthermore, the model fit is extremely poor. There appears to be some vertical asymptote.

Our bootstrap parameter estimates are as follows.

```{r}

paste0("Bootstrapped Mean Coefficient, beta_0: ", mean(b_coef[, 1]))
paste0("Bootstrapped Mean Coefficient, beta_1: ", mean(b_coef[, 2]))
paste0("Bootstrapped Mean Coefficient, beta_2: ", mean(b_coef[, 3]))
paste0("Bootstrapped Mean Coefficient, beta_3: ", mean(b_coef[, 4]))

```

We see that these bootstrap coefficients are very different than those from the model fit on the entire dataset.

The bootstrap confidence intervals are as follows.

```{r}

paste0("beta_0 bootstrap ci: ", quantile(b_coef[, 1], probs = c(0.025, 0.975)))
paste0("beta_1 bootstrap ci: ", quantile(b_coef[, 2], probs = c(0.025, 0.975)))
paste0("beta_2 bootstrap ci: ", quantile(b_coef[, 3], probs = c(0.025, 0.975)))
paste0("beta_3 bootstrap ci: ", quantile(b_coef[, 4], probs = c(0.025, 0.975)))

```

We see that the bootstrap confidence intervals for teh coefficients are extremely high. Additionally, two of the bootstrap confidence intervals contain 0, so we cannot claim that they are statistically significant. Note that the `b` parameter that appears statistically significant in the above output is no longer significant here. However, the `d`. `e` parameters are statistically significant, unlike the above output.

The following code gives measures of model fit.

```{r}

paste0("Deviance: ", deviance(best))
paste0("AIC: ", AIC(best))
paste0("BIC: ", BIC(best))

```

We see that compared to Model 1, there is smaller deviance, but increased AIC and BIC.

### Model 3

\[
y = \frac{\beta_0}{1 + exp(\beta_1 + \beta_2x)} + \epsilon
\]

We next again consider Model 1 and try to fit the robust linear regression

```{r, fig.cap="Robust nonlinear regression model fit for the population growth model.", warning=FALSE}

# storage
b_coef <- matrix(data = NA, ncol = 3, nrow = 2000)
ss_res <- rep(0, 2000)

for(i in 1:2000){

  boot <- sample(1:7, 7, replace = TRUE)
  boot_sample <- obs[boot, ]

  try({

    # logistic growth model
    nlfit <- nlrob(formula = y ~ a / (1 + exp(b + k * x)),
                  start = list(a = 100, b = 2.5, k = -0.1),
                  data = boot_sample,
                  control = nls.control(maxiter = 1e3,
                                    minFactor = .Machine$double.eps,
                                    tol = 1e-5),
                  algorithm = "default")
    
    b_coef[i, ] <- coefficients(nlfit)
    ss_res[i] <- sum(residuals(nlfit)^2)
    
  }, silent = TRUE)

}

b_coef <- unique(na.omit(b_coef))
ss_res <- unique(na.omit(ss_res))

best <- nlrob(formula = y ~ a / (1 + exp(b + k * x)),
             start = list(a = 100, b = 2.5, k = -0.1),
             data = obs,
             control = nls.control(maxiter = 1e3,
                                    minFactor = .Machine$double.eps,
                                    tol = 1e-5),
             algorithm = "default")

# function for predictions
my_func1 <- function(boot_co, dat){
  
  a <- mean(boot_co[, 1])
  b <- mean(boot_co[, 2])
  c <- mean(boot_co[, 3])
  pckg <- a / (1 + exp(b + c*dat))
  return(pckg)
  
}

# plots
pts <- seq(min(obs$x), max(obs$x), by = 0.01)
preds <- my_func1(boot_co = b_coef, dat = pts)
plot(obs$x, obs$y, xlab = "Range", ylab = "Efficiency", main = "Population Growth Model Fit")
lines(pts, preds, lty = 1, col = "red", lwd = 3)

```

```{r}

summary(best)

```

We see that the all three parameters are statistically significant when we fit the robust nonlinear regression on the entire dataset with this assumed model. Furthermore, the fitted robust nonlinear regression line appears to fit well to the data.

Our bootstrap parameter estimates are as follows.

```{r}

paste0("Bootstrapped Mean Coefficient, beta_0: ", mean(b_coef[, 1]))
paste0("Bootstrapped Mean Coefficient, beta_1: ", mean(b_coef[, 2]))
paste0("Bootstrapped Mean Coefficient, beta_2: ", mean(b_coef[, 3]))

```

We see that the bootstrap parameter estimates are comparable to those of the model fit with the entire dataset. Note that compared to Model 1, the parameters estiamtes are different. 

The bootstrap confidence intervals are the following.

```{r}

paste0("beta_0 bootstrap ci: ", quantile(b_coef[, 1], probs = c(0.025, 0.975)))
paste0("beta_1 bootstrap ci: ", quantile(b_coef[, 2], probs = c(0.025, 0.975)))
paste0("beta_2 bootstrap ci: ", quantile(b_coef[, 3], probs = c(0.025, 0.975)))

```

We see that none of the bootstrap confidence intervals contain zero, so that the bootstrap coefficients are significantly different from zero. Note that each bootstrap confidence intervals contain the parameter estimates from Model 1, so that the robust versus non-robust fits are not significantly different. 

The following code gives measures of model fit.

```{r}

paste0("Deviance: ", deviance(best))
paste0("AIC: ", AIC(best))
paste0("BIC: ", BIC(best))

```

Compared to Models 1 and 2, we have significantly smaller deviance, and smaller AIC and BIC.

### Model 4

\[
y = \beta_0 + x^{\beta_1} + \epsilon
\]

We wanted to think of a more simple model that captures the slow curvature of the data. Thus we consider an intercept plus some fractional power of the data. We expected it to increase logarithmically and may be an appropriate choice for this data.

```{r, fig.cap="Nonlinear regression model fit for the fractional power model."}

# storage
b_coef <- matrix(data = NA, ncol = 2, nrow = 2000)
ss_res <- rep(0, 2000)

for(i in 1:2000){

  boot <- sample(1:7, 7, replace = TRUE)
  boot_sample <- obs[boot, ]

  try({

    # logistic growth model
    nlfit <- nls2(formula = y ~ a + x^b,
                  start = list(a = 20, b = 0.5),
                  data = boot_sample,
                  control = nls.control(maxiter = 1e3,
                                    minFactor = .Machine$double.eps,
                                    tol = 1e-5),
                  algorithm = "default")
    
    b_coef[i, ] <- coefficients(nlfit)
    ss_res[i] <- sum(residuals(nlfit)^2)
    
  }, silent = TRUE)

}

b_coef <- unique(na.omit(b_coef))
ss_res <- unique(na.omit(ss_res))

best <- nls2(formula = y ~ a + x^b,
             start = list(a = 100, b = 0.5),
             data = obs,
             control = nls.control(maxiter = 1e3,
                                    minFactor = .Machine$double.eps,
                                    tol = 1e-5),
             algorithm = "default")

# function for predictions
my_func1 <- function(boot_co, dat){
  
  a <- mean(boot_co[, 1])
  b <- mean(boot_co[, 2])
  pckg <- a + dat^b
  return(pckg)
  
}

# plots
pts <- seq(min(obs$x), max(obs$x), by = 0.01)
preds <- my_func1(boot_co = b_coef, dat = pts)
plot(obs$x, obs$y, xlab = "Range", ylab = "Efficiency", main = "Population Growth Model Fit")
lines(pts, preds, lty = 1, col = "red", lwd = 3)

```

```{r}

summary(best)

```

We see that this model does not quite capture the nonlinear relationship in the dat. We do see that for the entire dataset, the `b` parameter is significantly different from 0.

Our bootstrap parameter estimates are as follows.

```{r}

paste0("Bootstrapped Mean Coefficient, beta_0: ", mean(b_coef[, 1]))
paste0("Bootstrapped Mean Coefficient, beta_1: ", mean(b_coef[, 2]))

```

We see that the bootstrap parameter estimates are comparable to those of the model fit with the entire dataset.

The bootstrap confidence intervals are the following.

```{r}

paste0("beta_0 bootstrap ci: ", quantile(b_coef[, 1], probs = c(0.025, 0.975)))
paste0("beta_1 bootstrap ci: ", quantile(b_coef[, 2], probs = c(0.025, 0.975)))

```

We see that the bootstrap confidence interval for the `a` parameter contains zero and reflects the summary above. The bootstrap confidence interval for `b` does not contain zero so it is significantly different from zero.

The following code gives measures of model fit.

```{r}

paste0("Deviance: ", deviance(best))
paste0("AIC: ", AIC(best))
paste0("BIC: ", BIC(best))

```

Not surprisingly, this model has an extremely high deviance measure and is a much worse fit compared to the previous models. However, the AIC and BIC measures are comparable still.

### Model 4

We next fit a spline model with cross-validation on the number of kinks. This better fit the entire dataset instead of the robust nonlinear population growth model.

```{r, fig.cap="Spline fit for Efficiency Versus Range"}

# storage
b_coef <- matrix(data = NA, ncol = 4, nrow = 2000)

for(i in 1:2000){

  boot <- sample(1:7, 7, replace = TRUE)
  boot_sample <- obs[boot, ]

  try({

    nlfit <- lm(y~bs(x), data = boot_sample)
    b_coef[i, ] <- coefficients(nlfit)
    
  }, silent = TRUE)

}

b_coef <- unique(na.omit(b_coef))

best <- lm(y~bs(x), data = obs)

# plots
plot(obs$x, obs$y, xlab = "Range", ylab = "Efficiency", main = "Spline Fit")
points(obs$x, best$fitted.values, col = 'red')

```

```{r}

summary(best)

```

We see that only one of the coefficients is statistically significant. The fitted values also it the data well except for the two x values stacked on top of each other. Here the spline fit simply takes the average efficiency between the two points.

Our bootstrap parameter estimates are as follows.

```{r}

paste0("Bootstrapped Mean Coefficient, beta_0: ", mean(b_coef[, 1]))
paste0("Bootstrapped Mean Coefficient, beta_1: ", mean(b_coef[, 2]))
paste0("Bootstrapped Mean Coefficient, beta_2: ", mean(b_coef[, 3]))
paste0("Bootstrapped Mean Coefficient, beta_3: ", mean(b_coef[, 4]))

```

We see that they are all significantly different from those estimated in the full model. This must have to do with the outlier point and the sampling procedure.

The bootstrap confidence intervals are the following.

```{r}

paste0("beta_0 bootstrap ci: ", quantile(b_coef[, 1], probs = c(0.025, 0.975)))
paste0("beta_1 bootstrap ci: ", quantile(b_coef[, 2], probs = c(0.025, 0.975)))
paste0("beta_2 bootstrap ci: ", quantile(b_coef[, 3], probs = c(0.025, 0.975)))
paste0("beta_3 bootstrap ci: ", quantile(b_coef[, 4], probs = c(0.025, 0.975)))

```

We see that the bootstrap confidence intervals match the significance results from fitting the full model. Only the $\beta_3$ coefficent is statistically significantly different from zero.

The following code gives measures of model fit.

```{r}

paste0("Deviance: ", deviance(best))
paste0("AIC: ", AIC(best))
paste0("BIC: ", BIC(best))

```

We see that they are an improvement over Models 1, 2, and 4, but not as good as Model 3.


## Model Discussion

We had various success fitting nonlinear models to the above data. Aside from choosing appropriate starting values for the assumed underlying models, it was extremely difficult bootstrapping the data to get appropriate estimates. We used a clever `try` statement inside our code and increased the bootstrap iterations to combat this process. 

Our most successful models were the population growth Model 1, spline Model 4, and robust population growth Model 3. Based on deviance, AIC, BIC measures we choose Model 3 as the best fitting model. It combats the outlier observation and still provides an appropraite fit for the majority of the data.


# Problem 2

Our data consists of two two herbacides appled at seven different rates with the dry weight percentages of velvetleaf plants are recorded. 

```{r}

# load data
raw <- read.csv("Herbicide.csv")
colnames(raw) <- NULL
raw <- raw[2:nrow(raw), ]
raw <- raw[, 2:ncol(raw)]
obs <- raw[, 1:2]
x <- as.numeric(obs[, 1])
y <- as.numeric(obs[, 2])

# simple linear regression fit
lmfit <- lm(y ~ x, data = obs)
cook <- cooks.distance(lmfit)

```

```{r, fig.cap="Scatter plot of Herbicide 1 data. There appears to be an exponential decay relationship between x and y."}

plot(x, y, xlab = "x", ylab = "y", main = "Scatter plot of Herbicide Observation 1")

```

```{r, fig.cap="Histogram of residuals from linear fit. The residuals do not appear normally distributed."}

hist(residuals(lmfit), main = "Residuals From Linear Fit", xlab = "Residuals")

```

```{r}

runs.test(residuals(lmfit))

```

Result of Runs Test. We fail to reject the null, so the residuals are iid.

```{r}

shapiro.test(residuals(lmfit))

```

Result of Shapiro-Wilk test for normality. Surprisingly, the residuals are normally distributed.

```{r}

bptest(lmfit)

```

Additionally, the data satisfy constant variance.

```{r, fig.cap="Various Linear Model Diagnostic Plots."}

par(mfrow=c(2,2))
halfnorm(cook, 3, ylab = "Cooks Distance", main = "Residual Cooks Distance")
plot(fitted(lmfit), residuals(lmfit), xlab = "Fitted", ylab = "Residuals", main = "Residuals Versus Fitted")
abline(h=0)
plot(fitted(lmfit), abs(residuals(lmfit)), xlab = "Fitted", ylab = "|Residuals|", main = "|Residuals| Versus Fitted")
abline(h=0)
qqnorm(residuals(lmfit))
qqline(residuals(lmfit))

```

In the simple linear regression setting, it appears that we do have a single outlier. Furthermore, all of our simple linear regression diagnostics hold. So how well does a simple linear regression line fit? 

```{r, fig.cap="Fitted regression line for Herbicide 1. We see that the regression line does not follow the curvature."}

summary(lmfit)
paste0("Deviance: ", deviance(lmfit))
paste0("AIC: ", AIC(lmfit))
paste0("BIC: ", BIC(lmfit))
plot(x, y, xlab = "x", ylab = "y", main = "Scatter plot of Herbicide 1")
abline(lmfit)

```

From the linear model summary output, we see that both coefficients are significant, as is the F-statistic. The Adjusted R-squared surprisingly good. We also get a deviance measure of the model fit with the AIC and BIC criterion. 

The above results show that a linear model may appear to be an appropriate choice for this data. However, we think a nonlinear model may fit the data better.

Our assumed model is 

\[
y_{i,j} = \alpha_j \frac{\beta_j x_{i,j}^{\gamma_j}}{1+\beta_j x_{i,j}^{\gamma_j}} + \epsilon.
\]

## Herbacide 1

```{r}

# choose obs
obs <- raw[, 1:2]
x <- as.numeric(obs[, 1])
y <- as.numeric(obs[, 2])

# storage
b_coef <- matrix(data = NA, ncol = 3, nrow = 2000)

# call
for(i in 1:2000){
  
  boot <- sample(1:7, 7, replace = TRUE)
  x_star <- x[boot]
  y_star <- y[boot]
  
  try({
    
    nlfit <- nls2(y_star ~ (p1 * p2 * x_star^p3) / (1 + p2 * x_star^p3), 
                start = c(p1 = 100, p2 = 1, p3 = -1),
                control = nls.control(maxiter = 1e3,
                                     minFactor = .Machine$double.eps,
                                     tol = 1e-5),
                algorithm = "default")
    
    b_coef[i, ] <- coefficients(nlfit)
    ss_res[i] <- sum(residuals(nlfit)^2)
    
  }, silent = TRUE)
  
}

# grab unique
b_coef <- unique(na.omit(b_coef))

# compare best estimate from all 7 data points to mean of bootstrapped sample
best <- nls2(y ~ (p1 * p2 * x^p3) / (1 + p2 * x^p3), 
              start = c(p1 = 100, p2 = 1, p3 = -1),
              control = nls.control(maxiter = 1e3,
                                    minFactor = .Machine$double.eps,
                                    tol = 1e-5),
              algorithm = "default")

summary(best)

# plot
plot(x, y, main = "Nonlinear fit for Herbicide Observation 1")
pts <- seq(min(x), max(x), by = 0.01)
preds <- predict(best, pts)
lines(x, preds, lty = 1, col = "red", lwd = 3)

```

We see that all three parameters are significantly different from zero. The fitted regression curve also appropriately fits the data.

The mean bootstrap coefficients are as follows.

```{r}

paste0("Bootstrapped Mean Coefficient, beta_0: ", mean(b_coef[, 1]))
paste0("Bootstrapped Mean Coefficient, beta_1: ", mean(b_coef[, 2]))
paste0("Bootstrapped Mean Coefficient, beta_2: ", mean(b_coef[, 3]))

```

We see that the mean bootstrap coefficients are comparable to the model fit on the entire dataset.

The bootsrap confidence intervals are as follows.

```{r}

paste0("beta_0 bootstrap ci: ", quantile(b_coef[, 1], probs = c(0.025, 0.975)))
paste0("beta_1 bootstrap ci: ", quantile(b_coef[, 2], probs = c(0.025, 0.975)))
paste0("beta_2 bootstrap ci: ", quantile(b_coef[, 3], probs = c(0.025, 0.975)))

```

We see that none of the intervals contain zero, reflecting the significant in the full model above. Additionally, each interval contains the estimated coefficients from the full model.

We have the following deviance measures.

```{r}

paste0("Deviance: ", deviance(best))
paste0("AIC: ", AIC(best))
paste0("BIC: ", BIC(best))

```

## Herbicide 2

```{r}

# choose obs
obs <- raw[, 3:4]
x <- as.numeric(obs[, 1])
y <- as.numeric(obs[, 2])

# storage
b_coef <- matrix(data = NA, ncol = 3, nrow = 2000)

# call
for(i in 1:2000){
  
  boot <- sample(1:7, 7, replace = TRUE)
  x_star <- x[boot]
  y_star <- y[boot]
  
  try({
    
    nlfit <- nls2(y_star ~ (p1 * p2 * x_star^p3) / (1 + p2 * x_star^p3), 
                start = c(p1 = 100, p2 = 1, p3 = -1),
                control = nls.control(maxiter = 1e3,
                                     minFactor = .Machine$double.eps,
                                     tol = 1e-5),
                algorithm = "default")
    
    b_coef[i, ] <- coefficients(nlfit)
    ss_res[i] <- sum(residuals(nlfit)^2)
    
  }, silent = TRUE)
  
}

# grab unique
b_coef <- unique(na.omit(b_coef))

# compare best estimate from all 7 data points to mean of bootstrapped sample
best <- nls2(y ~ (p1 * p2 * x^p3) / (1 + p2 * x^p3), 
              start = c(p1 = 100, p2 = 1, p3 = -1),
              control = nls.control(maxiter = 1e3,
                                    minFactor = .Machine$double.eps,
                                    tol = 1e-5),
              algorithm = "default")

summary(best)

# plot
plot(x, y, main = "Nonlinear fit for Herbicide Observation 1")
pts <- seq(min(x), max(x), by = 0.01)
preds <- predict(best, pts)
lines(x, preds, lty = 1, col = "red", lwd = 3)

```

Similar to the first observation, the fitted regression line follows the curvature in the second observation well.

The mean bootstrap coefficients are as follows.

```{r}

paste0("Bootstrapped Mean Coefficient, beta_0: ", mean(b_coef[, 1]))
paste0("Bootstrapped Mean Coefficient, beta_1: ", mean(b_coef[, 2]))
paste0("Bootstrapped Mean Coefficient, beta_2: ", mean(b_coef[, 3]))

```

Again, they are comparable to those estimated in the full model fit.

The bootstrap confidence intervals are as follows.

```{r}

paste0("beta_0 bootstrap ci: ", quantile(b_coef[, 1], probs = c(0.025, 0.975)))
paste0("beta_1 bootstrap ci: ", quantile(b_coef[, 2], probs = c(0.025, 0.975)))
paste0("beta_2 bootstrap ci: ", quantile(b_coef[, 3], probs = c(0.025, 0.975)))

```

Similar to the first observation, they do not contain zero and contain the estimated parameters from the full model. Thus the bootstrap estimated coefficients are significantly different from zero.

The following are the measures of model fit.

```{r}

paste0("Deviance: ", deviance(best))
paste0("AIC: ", AIC(best))
paste0("BIC: ", BIC(best))

```

We see that this model for the second observation performs slightly worse than the first observation.

