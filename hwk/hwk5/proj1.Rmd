---
title: "Project 1"
subtitle: "Regression Analysis"
author: "Alexander Durbin"
date: "`r Sys.Date()`"
output: html_document
abstract: "This paper concerns itself with nonlinear regression. We look at two different datasets where either the true underlying model is known or unknown. We compare multiple models with the deviance measure, AIC, and BIC. We then make inference with boostrap confidence intervals. On this basis we choose a best fitting model."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = FALSE)
```

```{r, include=FALSE}
options(stringsAsFactors = FALSE)
library(minpack.lm)
library(nls2)
library(mgcv)
library(robustbase)
library(faraway)
library(lawstat)
library(lmtest)
```

# Problem 1

## Introduction

Our data concists of the efficiency for krigging of different soil attributes as a function of the spatial correlation range. Specific soil attributes are 1:1 soil water mixture, extractable soil phosphorus, calcium extractable, cation exchange capacity, and lime and P fertilizer recommendations. We begin with a scatter plot and simple regression diagnostics.

```{r}

raw <- read.csv("EffRange.csv")
raw <- raw[, 2:ncol(raw)]
obs <- raw[, 1:2]
colnames(obs) <- c('y', 'x')

# simple linear regression fit
lmfit <- lm(y ~ x, data = obs)
cook <- cooks.distance(lmfit)

```

```{r, fig.cap="Scatter plot of Range versus Efficiency. There is clearly a nonlinear relationship between the two variables."}

plot(obs$x, obs$y, xlab = "Range", ylab = "Efficiency", main = "Scatter plot of Range Versus Efficiency")

```

```{r, fig.cap="Histogram of residuals from linear fit. The residuals do not appear normally distributed."}

hist(residuals(lmfit), main = "Residuals From Linear Fit", xlab = "Residuals")

```

```{r}

runs.test(residuals(lmfit))

```

Result of Runs Test. We fail to reject the null, so the residuals are iid.

```{r}

shapiro.test(residuals(lmfit))

```

Result of Shapiro-Wilk test for normality. Surprisingly, the residuals are normally distributed.

```{r}

bptest(lmfit)

```

Additionally, the data satisfy constant variance.

```{r, fig.cap="Various Linear Model Diagnostic Plots."}

par(mfrow=c(2,2))
halfnorm(cook, 3, ylab = "Cooks Distance", main = "Residual Cooks Distance")
plot(fitted(lmfit), residuals(lmfit), xlab = "Fitted", ylab = "Residuals", main = "Residuals Versus Fitted")
abline(h=0)
plot(fitted(lmfit), abs(residuals(lmfit)), xlab = "Fitted", ylab = "|Residuals|", main = "|Residuals| Versus Fitted")
abline(h=0)
qqnorm(residuals(lmfit))
qqline(residuals(lmfit))

```

In the simple linear regression setting, it appears that we do havee a single outlier. Furthermore, all of our simple linear regression diagnostics hold. So how well does a simple linear regression line fit? 

```{r, fig.cap="Fitted regression line predicting Efficiency from Range. We see that the regression line does not follow the curvature."}

summary(lmfit)
paste0("Deviance: ", deviance(lmfit))
paste0("AIC: ", AIC(lmfit))
paste0("BIC: ", BIC(lmfit))
plot(obs$x, obs$y, xlab = "Range", ylab = "Efficiency", main = "Scatter plot of Range Versus Efficiency")
abline(lmfit)

```

From the linear model summary output, we see that neither coefficients are significant, the F-statistic is not significant, and the Adjusted R-squared is very poor. We also get a deviance measure of the model fit with the AIC and BIC criterion. 

The above results show that a linear model is not an appropriate choice for this data.

### Model 1

\[
y = \frac{\beta_0}{1 + exp(\beta_1 + \beta_2x)} + \epsilon
\]

The first model we consider is an exponential growth model. It has several nice properties that suggest it is an appropriate choice and is similar to a logistic model. Here, $\beta_0$ is the horizontal asymptote, $\beta_1$ is the y-intercept, and $\beta_2$ is the growth rate. We make visual guesses of these three parameters before fitting the nonlinear regression based on this model. We give three measures of model fit and confidence intervals for the coefficients. 

```{r, fig.cap="Nonlinear regression model fit for the population growth model."}

# storage
b_coef <- matrix(data = NA, ncol = 3, nrow = 2000)
ss_res <- rep(0, 2000)

for(i in 1:2000){

  boot <- sample(1:7, 7, replace = TRUE)
  boot_sample <- obs[boot, ]

  try({

    # logistic growth model
    nlfit <- nls2(formula = y ~ a / (1 + exp(b + k * x)),
                  start = list(a = 100, b = 2.5, k = -0.1),
                  data = boot_sample,
                  control = nls.control(maxiter = 1e3,
                                    minFactor = .Machine$double.eps,
                                    tol = 1e-5),
                  algorithm = "default")
    
    b_coef[i, ] <- coefficients(nlfit)
    ss_res[i] <- sum(residuals(nlfit)^2)
    
  }, silent = TRUE)

}

b_coef <- unique(na.omit(b_coef))
ss_res <- unique(na.omit(ss_res))

best <- nls2(formula = y ~ a / (1 + exp(b + k * x)),
             start = list(a = 100, b = 2.5, k = -0.1),
             data = obs,
             control = nls.control(maxiter = 1e3,
                                    minFactor = .Machine$double.eps,
                                    tol = 1e-5),
             algorithm = "default")

# function for predictions
my_func1 <- function(boot_co, dat){
  
  a <- mean(boot_co[, 1])
  b <- mean(boot_co[, 2])
  c <- mean(boot_co[, 3])
  pckg <- a / (1 + exp(b + c*dat))
  return(pckg)
  
}

# plots
pts <- seq(min(obs$x), max(obs$x), by = 0.01)
preds <- my_func1(boot_co = b_coef, dat = pts)
plot(obs$x, obs$y, xlab = "Range", ylab = "Efficiency", main = "Population Growth Model Fit")
lines(pts, preds, lty = 1, col = "red", lwd = 3)

```

```{r}

summary(best)

```

We see that the `a` parameter is statistically significant when we fit the nonlinear regression on the entire dataset with this assumed model. Furthermore, the fitted nonlinear regression line appears to fit well to the data.

Our bootstrap parameter estimates are as follows.

```{r}

paste0("Bootstrapped Mean Coefficient, beta_0: ", mean(b_coef[, 1]))
paste0("Bootstrapped Mean Coefficient, beta_1: ", mean(b_coef[, 2]))
paste0("Bootstrapped Mean Coefficient, beta_2: ", mean(b_coef[, 3]))

```

We see that the bootstrap parameter estimates are comparable to those of the model fit with the entire dataset.

The bootstrap confidence intervals are the following.

```{r}

paste0("beta_0 bootstrap ci: ", quantile(b_coef[, 1], probs = c(0.025, 0.975)))
paste0("beta_1 bootstrap ci: ", quantile(b_coef[, 2], probs = c(0.025, 0.975)))
paste0("beta_2 bootstrap ci: ", quantile(b_coef[, 3], probs = c(0.025, 0.975)))

```

We see that none of the bootstrap confidence intervals contain zero, so that the bootstrap coefficients are significantly different from zero. Note that the `a` parameter is statistically significant in the model fit with the entire dataset.

The following code gives measures of model fit.

```{r}

paste0("Deviance: ", deviance(best))
paste0("AIC: ", AIC(best))
paste0("BIC: ", BIC(best))

```

### Model 2

\[
y = \frac{\beta_0(x^2+x\beta_1)}{x^2+x\beta_2+\beta_3} + \epsilon
\]

This model was chosen based on nonlinear example datasets from the *Information Technology Laboratory* website. It appear does also appear to follow a similar logistic regression path. We fit this model and give measurements of fit as well as confidence intervals.

```{r, fig.cap="Nonlinear regression model fit for the modified population growth model."}

# storage
b_coef <- matrix(data = NA, ncol = 4, nrow = 2000)
ss_res <- rep(0, 2000)

for(i in 1:2000){

  boot <- sample(1:7, 7, replace = TRUE)
  boot_sample <- obs[boot, ]

  try({

    # logistic growth model
    nlfit <- nls2(formula = y ~ a*(x^2+b*x)/(x^2+x*d+e),
             start = list(a = 32.6, b = -40, d = -63, e = 1650),
             data = boot_sample,
             control = nls.control(maxiter = 1e3,
                                    minFactor = .Machine$double.eps,
                                    tol = 1e-5),
             algorithm = "default")
    
    b_coef[i, ] <- coefficients(nlfit)
    ss_res[i] <- sum(residuals(nlfit)^2)
    
  }, silent = TRUE)

}

b_coef <- unique(na.omit(b_coef))
ss_res <- unique(na.omit(ss_res))

best <- nls2(formula = y ~ a*(x^2+b*x)/(x^2+x*d+e),
             start = list(a = 32.6, b = -40, d = -63, e = 1650),
             data = obs,
             control = nls.control(maxiter = 1e3,
                                    minFactor = .Machine$double.eps,
                                    tol = 1e-5),
             algorithm = "default")

# function for predictions
my_func1 <- function(boot_co, dat){

  a <- mean(boot_co[, 1])
  b <- mean(boot_co[, 2])
  d <- mean(boot_co[, 3])
  e <- mean(boot_co[, 4])
  pckg <- a*(dat^2 + dat*b) / (dat^2 + dat*d+e)
  return(pckg)

}

# plots
pts <- seq(min(obs$x), max(obs$x), by = 0.01)
preds <- my_func1(boot_co = b_coef, dat = pts)
plot(obs$x, obs$y, xlab = "Range", ylab = "Efficiency", main = "Modified Population Growth Model Fit")
# abline(best)
lines(pts, preds, lty = 1, col = "red", lwd = 3)

```

```{r}

summary(best)

```

We see that only the `b` parameter is statistically significant when this model is fit to the entire dataset. Furthermore, the model fit is extremely poor. There appears to be some vertical asymptote.

Our bootstrap parameter estimates are as follows.

```{r}

paste0("Bootstrapped Mean Coefficient, beta_0: ", mean(b_coef[, 1]))
paste0("Bootstrapped Mean Coefficient, beta_1: ", mean(b_coef[, 2]))
paste0("Bootstrapped Mean Coefficient, beta_2: ", mean(b_coef[, 3]))
paste0("Bootstrapped Mean Coefficient, beta_3: ", mean(b_coef[, 4]))

```

We see that these bootstrap coefficients are very different than those from the model fit on the entire dataset.

The bootstrap confidence intervals are as follows.

```{r}

paste0("beta_0 bootstrap ci: ", quantile(b_coef[, 1], probs = c(0.025, 0.975)))
paste0("beta_1 bootstrap ci: ", quantile(b_coef[, 2], probs = c(0.025, 0.975)))
paste0("beta_2 bootstrap ci: ", quantile(b_coef[, 3], probs = c(0.025, 0.975)))
paste0("beta_3 bootstrap ci: ", quantile(b_coef[, 4], probs = c(0.025, 0.975)))

```

We see that the bootstrap confidence intervals for teh coefficients are extremely high. Additionally, two of the bootstrap confidence intervals contain 0, so we cannot claim that they are statistically significant. Note that the `b` parameter that appears statistically significant in the above output is no longer significant here. However, the `d`. `e` parameters are statistically significant, unlike the above output.

The following code gives measures of model fit.

```{r}

paste0("Deviance: ", deviance(best))
paste0("AIC: ", AIC(best))
paste0("BIC: ", BIC(best))

```

We see that compared to Model 1, there is smaller deviance, but increased AIC and BIC.

### Model 3

\[
y = \frac{\beta_0}{1 + exp(\beta_1 + \beta_2x)} + \epsilon
\]

We next again consider Model 1 and try to fit the robust linear regression

```{r, fig.cap="Robust nonlinear regression model fit for the population growth model.", warning=FALSE}

# storage
b_coef <- matrix(data = NA, ncol = 3, nrow = 2000)
ss_res <- rep(0, 2000)

for(i in 1:2000){

  boot <- sample(1:7, 7, replace = TRUE)
  boot_sample <- obs[boot, ]

  try({

    # logistic growth model
    nlfit <- nlrob(formula = y ~ a / (1 + exp(b + k * x)),
                  start = list(a = 100, b = 2.5, k = -0.1),
                  data = boot_sample,
                  control = nls.control(maxiter = 1e3,
                                    minFactor = .Machine$double.eps,
                                    tol = 1e-5),
                  algorithm = "default")
    
    b_coef[i, ] <- coefficients(nlfit)
    ss_res[i] <- sum(residuals(nlfit)^2)
    
  }, silent = TRUE)

}

b_coef <- unique(na.omit(b_coef))
ss_res <- unique(na.omit(ss_res))

best <- nlrob(formula = y ~ a / (1 + exp(b + k * x)),
             start = list(a = 100, b = 2.5, k = -0.1),
             data = obs,
             control = nls.control(maxiter = 1e3,
                                    minFactor = .Machine$double.eps,
                                    tol = 1e-5),
             algorithm = "default")

# function for predictions
my_func1 <- function(boot_co, dat){
  
  a <- mean(boot_co[, 1])
  b <- mean(boot_co[, 2])
  c <- mean(boot_co[, 3])
  pckg <- a / (1 + exp(b + c*dat))
  return(pckg)
  
}

# plots
pts <- seq(min(obs$x), max(obs$x), by = 0.01)
preds <- my_func1(boot_co = b_coef, dat = pts)
plot(obs$x, obs$y, xlab = "Range", ylab = "Efficiency", main = "Population Growth Model Fit")
lines(pts, preds, lty = 1, col = "red", lwd = 3)

```

```{r}

summary(best)

```

We see that the all three parameters are statistically significant when we fit the robust nonlinear regression on the entire dataset with this assumed model. Furthermore, the fitted robust nonlinear regression line appears to fit well to the data.

Our bootstrap parameter estimates are as follows.

```{r}

paste0("Bootstrapped Mean Coefficient, beta_0: ", mean(b_coef[, 1]))
paste0("Bootstrapped Mean Coefficient, beta_1: ", mean(b_coef[, 2]))
paste0("Bootstrapped Mean Coefficient, beta_2: ", mean(b_coef[, 3]))

```

We see that the bootstrap parameter estimates are comparable to those of the model fit with the entire dataset. Note that compared to Model 1, the parameters estiamtes are different. 

The bootstrap confidence intervals are the following.

```{r}

paste0("beta_0 bootstrap ci: ", quantile(b_coef[, 1], probs = c(0.025, 0.975)))
paste0("beta_1 bootstrap ci: ", quantile(b_coef[, 2], probs = c(0.025, 0.975)))
paste0("beta_2 bootstrap ci: ", quantile(b_coef[, 3], probs = c(0.025, 0.975)))

```

We see that none of the bootstrap confidence intervals contain zero, so that the bootstrap coefficients are significantly different from zero. Note that each bootstrap confidence intervals contain the parameter estimates from Model 1, so that the robust versus non-robust fits are not significantly different. 

The following code gives measures of model fit.

```{r}

paste0("Deviance: ", deviance(best))
paste0("AIC: ", AIC(best))
paste0("BIC: ", BIC(best))

```

Compared to Models 1 and 2, we have significantly smaller deviance, and smaller AIC and BIC.

### Model 4

\[
y = \beta_0 + x^{\beta_1} + \epsilon
\]

We wanted to think of a more simple model that captures the slow curvature of the data. Thus we consider an intercept plus some fractional power of the data. We expected it to increase logarithmically and may be an appropriate choice for this data.

```{r, fig.cap="Nonlinear regression model fit for the fractional power model."}

# storage
b_coef <- matrix(data = NA, ncol = 2, nrow = 2000)
ss_res <- rep(0, 2000)

for(i in 1:2000){

  boot <- sample(1:7, 7, replace = TRUE)
  boot_sample <- obs[boot, ]

  try({

    # logistic growth model
    nlfit <- nls2(formula = y ~ a + x^b,
                  start = list(a = 20, b = 0.5),
                  data = boot_sample,
                  control = nls.control(maxiter = 1e3,
                                    minFactor = .Machine$double.eps,
                                    tol = 1e-5),
                  algorithm = "default")
    
    b_coef[i, ] <- coefficients(nlfit)
    ss_res[i] <- sum(residuals(nlfit)^2)
    
  }, silent = TRUE)

}

b_coef <- unique(na.omit(b_coef))
ss_res <- unique(na.omit(ss_res))

best <- nls2(formula = y ~ a + x^b,
             start = list(a = 100, b = 0.5),
             data = obs,
             control = nls.control(maxiter = 1e3,
                                    minFactor = .Machine$double.eps,
                                    tol = 1e-5),
             algorithm = "default")

# function for predictions
my_func1 <- function(boot_co, dat){
  
  a <- mean(boot_co[, 1])
  b <- mean(boot_co[, 2])
  pckg <- a + dat^b
  return(pckg)
  
}

# plots
pts <- seq(min(obs$x), max(obs$x), by = 0.01)
preds <- my_func1(boot_co = b_coef, dat = pts)
plot(obs$x, obs$y, xlab = "Range", ylab = "Efficiency", main = "Population Growth Model Fit")
lines(pts, preds, lty = 1, col = "red", lwd = 3)

```

```{r}

summary(best)

```

We see that the `a` parameter is statistically significant when we fit the nonlinear regression on the entire dataset with this assumed model. Furthermore, the fitted nonlinear regression line appears to fit well to the data.

Our bootstrap parameter estimates are as follows.

```{r}

paste0("Bootstrapped Mean Coefficient, beta_0: ", mean(b_coef[, 1]))
paste0("Bootstrapped Mean Coefficient, beta_1: ", mean(b_coef[, 2]))

```

We see that the bootstrap parameter estimates are comparable to those of the model fit with the entire dataset.

The bootstrap confidence intervals are the following.

```{r}

paste0("beta_0 bootstrap ci: ", quantile(b_coef[, 1], probs = c(0.025, 0.975)))
paste0("beta_1 bootstrap ci: ", quantile(b_coef[, 2], probs = c(0.025, 0.975)))

```

We see that none of the bootstrap confidence intervals contain zero, so that the bootstrap coefficients are significantly different from zero. Note that the `a` parameter is statistically significant in the model fit with the entire dataset.

The following code gives measures of model fit.

```{r}

paste0("Deviance: ", deviance(best))
paste0("AIC: ", AIC(best))
paste0("BIC: ", BIC(best))

```
