---
title: 'STAT5514: Homework 2'
author: "Alexander Durbin"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
abstract: 'This simulation study compares several regression techniques across three different error distributions. We calculate the mean squared error of the coefficients and choose the two best methods for each error distribution. The two best methods are based on smallest mean squared error.'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
options(stringsAsFactors = FALSE, scipen = 1E8)
```

```{r, include = FALSE}

library(tidyverse)
library(tibble)
library(MASS)
library(quantreg)
library(galts)
library(knitr)

```

```{r, eval = TRUE, echo = FALSE}

b0_sim <- tibble(ls1 = rep(0, 100),
                 ls2 = rep(0, 100),
                 ls3 = rep(0, 100),
                 hb1 = rep(0, 100),
                 hb2 = rep(0, 100),
                 hb3 = rep(0, 100),
                 ld1 = rep(0, 100),
                 ld2 = rep(0, 100),
                 ld3 = rep(0, 100),
                 lt1 = rep(0, 100),
                 lt2 = rep(0, 100),
                 lt3 = rep(0, 100))

b1_sim <- tibble(ls1 = rep(0, 100),
                 ls2 = rep(0, 100),
                 ls3 = rep(0, 100),
                 hb1 = rep(0, 100),
                 hb2 = rep(0, 100),
                 hb3 = rep(0, 100),
                 ld1 = rep(0, 100),
                 ld2 = rep(0, 100),
                 ld3 = rep(0, 100),
                 lt1 = rep(0, 100),
                 lt2 = rep(0, 100),
                 lt3 = rep(0, 100))

b0 <- 1
b1 <- 2

for(i in 1:100){
  raw <- tibble(x = rnorm(100, 0, 1), 
                e1 = rnorm(100, 0, 2),
                e2 = rlnorm(100, 0, 2),
                e3 = stats::rcauchy(100, 0 ,2),
                y1 = b0 + b1 * x + e1,
                y2 = b0 + b1 * x + e2,
                y3 = b0 + b1 * x + e3)
  
  
  
  l1 <- lm(y1 ~ x, data = raw)
  l2 <- lm(y2 ~ x, data = raw)
  l3 <- lm(y3 ~ x, data = raw)
  
  b0_sim$ls1[i] = coefficients(l1)[1]
  b1_sim$ls1[i] = coefficients(l1)[2]
  
  b0_sim$ls2[i] = coefficients(l2)[1]
  b1_sim$ls2[i] = coefficients(l2)[2]
  
  b0_sim$ls3[i] = coefficients(l3)[1]
  b1_sim$ls3[i] = coefficients(l3)[2]
  
  l1 <- rlm(y1 ~ x, data = raw)
  l2 <- rlm(y2 ~ x, data = raw)
  l3 <- rlm(y3 ~ x, data = raw)
  
  b0_sim$hb1[i] = coefficients(l1)[1]
  b1_sim$hb1[i] = coefficients(l1)[2]
  
  b0_sim$hb2[i] = coefficients(l2)[1]
  b1_sim$hb2[i] = coefficients(l2)[2]
  
  b0_sim$hb3[i] = coefficients(l3)[1]
  b1_sim$hb3[i] = coefficients(l3)[2]
  
  l1 <- rq(y1 ~ x, data = raw)
  l2 <- rq(y2 ~ x, data = raw)
  l3 <- rq(y3 ~ x, data = raw)
  
  b0_sim$ld1[i] = coefficients(l1)[1]
  b1_sim$ld1[i] = coefficients(l1)[2]
  
  b0_sim$ld2[i] = coefficients(l2)[1]
  b1_sim$ld2[i] = coefficients(l2)[2]
  
  b0_sim$ld3[i] = coefficients(l3)[1]
  b1_sim$ld3[i] = coefficients(l3)[2]

  l1 <- ga.lts(raw$y1 ~ raw$x, lower = -100, upper = 100)
  l2 <- ga.lts(raw$y2 ~ raw$x, lower = -100, upper = 100)
  l3 <- ga.lts(raw$y3 ~ raw$x, lower = -100, upper = 100)
  
  b0_sim$lt1[i] = coefficients(l1)[1]
  b1_sim$lt1[i] = coefficients(l1)[2]
  
  b0_sim$lt2[i] = coefficients(l2)[1]
  b1_sim$lt2[i] = coefficients(l2)[2]
  
  b0_sim$lt3[i] = coefficients(l3)[1]
  b1_sim$lt3[i] = coefficients(l3)[2]
 
}


```


```{r, eval = TRUE, echo = FALSE}

b0_s_m <- colMeans(b0_sim)
b1_s_m <- colMeans(b1_sim)

b0_bias <- (b0 - b0_s_m)
b1_bias <- (b1 - b1_s_m)

b0_bias_sq <- (b0 - b0_s_m)^2
b1_bias_sq <- (b1 - b1_s_m)^2

b0_var <- sapply(b0_sim, var)
b1_var <- sapply(b1_sim, var)

b0_mse <- b0_bias_sq + b0_var

b1_mse <- b1_bias_sq + b1_var
```


# Results

```{r, eval = TRUE, echo=FALSE}

knitr::kable(as.data.frame(matrix(b0_var,nrow = 3), row.names = c('Normal', 'Lognormal', 'Cauchy')), col.names = c('LSE', 'RLM', 'LAD', 'LTS'), caption = 'Variance of intercept estimates.')

knitr::kable(as.data.frame(matrix(b1_var,nrow = 3), row.names = c('Normal', 'Lognormal', 'Cauchy')), col.names = c('LSE', 'RLM', 'LAD', 'LTS'), caption = 'Variance of slope estimates.')

knitr::kable(as.data.frame(matrix(b0_bias,nrow = 3), row.names = c('Normal', 'Lognormal', 'Cauchy')), col.names = c('LSE', 'RLM', 'LAD', 'LTS'), caption = 'Bias of intercept estimates.')

knitr::kable(as.data.frame(matrix(b1_bias,nrow = 3), row.names = c('Normal', 'Lognormal', 'Cauchy')), col.names = c('LSE', 'RLM', 'LAD', 'LTS'), caption = 'Bias of slope estimates.')

knitr::kable(as.data.frame(matrix(b0_mse,nrow = 3), row.names = c('Normal', 'Lognormal', 'Cauchy')), col.names = c('LSE', 'RLM', 'LAD', 'LTS'), caption = 'MSE of intercept estimates.')

knitr::kable(as.data.frame(matrix(b1_mse,nrow = 3), row.names = c('Normal', 'Lognormal', 'Cauchy')), col.names = c('LSE', 'RLM', 'LAD', 'LTS'), caption = 'MSE of slope estimates.')

```

# Normal Error - LSE, RLM

As expected for normally distributed error, lease squares performs best with minimum mean squarred error. Why is best understood using linear models theory. Under normally distributed error and constant variance, we know that least squares estimators are unbiased and have minimum variance amongst all unbiased linear estimators by Gauss-Markov Theorem.

Robust regression performs next best with results very comparable to least squares. Robust regression is most effective when there are several large residuals so that it may appropriately downweight these observations. But for normally distributed error with low variance, there are few outliers. It must be the case that the majority of observations are given full weight so that during the iterative reweighted least squares procedure, we are essentially performing least squares estimation.

# Lognormal Error - LTS, LAD

Least trimmed squares and least absolute deviation perform best in the lognormal error term case. Since the lognormal distribution is highly skewed, the median is a more robust estimator of the mean, so LAD should perform well here. Additionally, least trimmed squares has minimal MSE by far since it minimizes a subset of the sum of residuals.

# Cauchy Error - LAD, RLM

This is an ideal case where we would expect robust regression to perform well. Cauchy draws are highly variable and an extreme outlier is likely. Thus, robust regression appropriately downweights multiple outliers to produce estimates with next-smallest means squared error. Note that in some of the simulations, robust regression was either slightly outperformed by least trimmed squares or had comparable results. We attribute these slight differences to the Cauchy draws in our simulations and the useage of genetic functions in `ga.lts`.

Least absolute deviation has superior performance for Cauchy distributed error terms. I would have thought least trimmed squares would have performed better in this case because it would minimize the sum of squared residuals for a subset that did not include the outlier terms. The median is a more robust estimator of the mean when we have data with extreme observations, so this effect must be more relevant here apparent here.